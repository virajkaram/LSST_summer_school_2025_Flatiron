{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5ade5a-9d04-46ba-9567-859f21da24ca",
   "metadata": {},
   "source": [
    "# Searching for transients in localization regions - The case of GW190521\n",
    "\n",
    "Borrows from healpix tutorial notebooks by Leo Singer and tutorials for the Alerce broker. Prepared by Kishalay De (kde@flatironinstitute.org) for the LSST CCA Summer School 2025.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The goal of this notebook is to use real ZTF data to search for electromagnetic counterparts to gravitational wave sources. The corresponding localization regions are typically very large, spanning hundreds of square degrees requiring us to use spatial and temporal filtering techniques to identify counterparts of interest. The notebook will use the actual published localization region for the binary black hole merger event GW190521, which was identified as one of the most massive events ever detected ([Abbott et al. 2020](https://ui.adsabs.harvard.edu/abs/2020PhRvL.125j1102A/abstract)), together with a possible electromagnetic counterpart claimed in [Graham et al. 2021](https://ui.adsabs.harvard.edu/abs/2020PhRvL.124y1102G/abstract).\n",
    "\n",
    "Although counterparts are not typically expected for binary BH mergers, the techniques used in this notebook are meant to motivate general search criteria for filtering large alert streams and will show the steps that can lead to the identification of the proposed electromagnetic counterpart.\n",
    "\n",
    "## Required python packages (pip install)\n",
    "\n",
    "astropy, matplotlib, numpy, healpy, ligo.skymap, scipy, pandas, alerce, astroquery\n",
    "\n",
    "## Ancilliary datasets\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f2adac-096e-47db-9a0e-cab7389d1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import the modules we'll need\n",
    "\n",
    "import astropy.utils.data\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "\n",
    "from astropy.table import Table, vstack, hstack, Column\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "import ligo.skymap.plot\n",
    "from scipy.stats import norm\n",
    "import scipy.stats\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "from astropy.time import Time\n",
    "\n",
    "#this is our alert broker\n",
    "from alerce.core import Alerce\n",
    "\n",
    "from astroquery.vizier import Vizier\n",
    "\n",
    "from astropy.cosmology import z_at_value\n",
    "from astropy.cosmology import FlatLambdaCDM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b809baa-0975-40f8-a39b-a314330f693a",
   "metadata": {},
   "source": [
    "## HEALPix Basics\n",
    "\n",
    "This section on using HEALPix localization files is adapted from the [LIGO/Virgo Public Alerts User Guide](https://emfollow.docs.ligo.org/userguide/tutorial/skymaps.html). We'll start with first loading the Healpix localization for the event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6bfa2f-59f5-48bb-b9f3-c2eec1d2af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://gracedb.ligo.org/api/superevents/S190521g/files/GW190521_PublicationSamples_flattened.fits.gz,0'\n",
    "filename = astropy.utils.data.download_file(url)\n",
    "\n",
    "prob, distmu, distsigma, distnorm = hp.read_map(filename, field=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7800f-32c8-4d03-8a8d-46357e9b4943",
   "metadata": {},
   "source": [
    "To get a quick look at a HEALPix data set, you can use the `hp.mollview` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6408e-b4c4-4372-a8e7-b8338d1fe4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.mollview(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab9f05a-b8af-44df-9414-4773c0ac43e6",
   "metadata": {},
   "source": [
    "What actually is stored in `prob`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fceb6fa-2cd3-4195-bed5-5b6be7510512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print prob\n",
    "??????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c444c-510d-42f8-8217-6381d0bf864b",
   "metadata": {},
   "source": [
    "It's a one-dimensional array! Yet it represents in 2D image. How does that work? HEALPix is a way to *index* equal-area regions on the unit sphere using integers.\n",
    "\n",
    "To decode HEALPix indices, you need to know the resolution of the map, which is described by a parameter called `nside`. `nside` is the number of subdivisions of 12 base HEALPix tiles, so the relation between the length of a HEALPix array, `npix`, and its resolution, `nside`, is\n",
    "\n",
    "$$\n",
    "    \\mathsf{npix} = 12 \\cdot \\mathsf{nside}^2.\n",
    "$$\n",
    "\n",
    "The functions `hp.npix2nside` and `hp.nside2npix` convert between length and resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59dc439-ae22-4337-8aa7-7624c7b68e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "npix = len(prob)\n",
    "npix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f17a6-f846-46af-9083-fa56fce34359",
   "metadata": {},
   "outputs": [],
   "source": [
    "nside = hp.npix2nside(npix)\n",
    "nside"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80df560-1f28-4c1f-8d36-de72334e6768",
   "metadata": {},
   "source": [
    "The function `hp.pix2ang` allows us to convert from (ra, dec) and HEALPix pixel index.\n",
    "\n",
    "*Note*: by default, these functions return 'physics' spherical coordinates $(\\theta, \\phi)$ in radians, but you can switch to 'astronomy' spherical coordinates in degrees by passing the keyword argument `lonlat=True`.\n",
    "\n",
    "Let's look up the right ascension and declination of pixel 123."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8a4e3-ce0f-4171-9e73-ada759311e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipix = 123\n",
    "ra, dec = hp.pix2ang(nside, ipix, lonlat=True)\n",
    "ra, dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ce8dd-27eb-4082-9228-b9d694880071",
   "metadata": {},
   "source": [
    "The function `hp.ang2pix` does the opposite. Let's find the pixel that contains the point RA=194.95, Dec=27.98."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c56f85-7af8-436d-b729-265edfb04c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = 194.95\n",
    "dec = 27.98\n",
    "hp.ang2pix(nside, ra, dec, lonlat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036ccbf2-7839-40ec-bb93-0ebedfcd4c8d",
   "metadata": {},
   "source": [
    "What is the most probable sky location? Just find the pixel with the maximum value, and then find its right ascension and declination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d534530-cbce-4520-8bfd-3c17601817f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the maximum probability pixel\n",
    "ipix_max = ?????\n",
    "ipix_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29b223-9dec-4b88-86c4-f1c6a1087f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.pix2ang(nside, ipix_max, lonlat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67acbaa7-c888-47f6-850a-e883e3f34886",
   "metadata": {},
   "source": [
    "## Visualizing the localization region\n",
    "\n",
    "Consistent with the molleweide projection above, one finds that the peak of the probability distribution is in the southern hemisphere, which happens to be completely outside the visibility range for ZTF (!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a866e8-1ff6-44f6-98c2-b22ec416eeaf",
   "metadata": {},
   "source": [
    "Given this situation let's focus on the part of the localization that ZTF does see. We'll start by making a plot of the sky localization focussing on the northern lobe, i.e. declination > 0. ZTF can observe down to declination $\\approx -30^\\circ$, but we see that the southern lobe is much further south than that; so we'll ignore that region. Of particular interest are the localization contours corresponding to the 50% and 90% localization regions, which we can calculate by asking which areas of the sky accumulate a total of the corresponding probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e301911-f354-4be3-bdd4-8c5943a05443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort pixels in descending order of probability\n",
    "sorted_prob = np.sort(prob[prob > 0])[::-1]\n",
    "cum_prob = np.cumsum(sorted_prob)\n",
    "\n",
    "# Find thresholds for 50% and 90% credible regions\n",
    "p50 = sorted_prob[np.searchsorted(cum_prob, 0.5)]\n",
    "p90 = sorted_prob[np.searchsorted(cum_prob, 0.9)]\n",
    "\n",
    "# Get pixels in 90% region\n",
    "pix_90 = np.where(prob >= p90)[0]\n",
    "ra_all, dec_all = hp.pix2ang(nside, pix_90, lonlat=True)\n",
    "\n",
    "# Create mask map for contouring\n",
    "# Higher probability → lower values in the contour function \n",
    "contour_map = np.copy(prob)\n",
    "contour_map[prob == hp.UNSEEN] = 0 \n",
    "\n",
    "# Define axes with astro projection\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "ax = plt.axes([0.05, 0.05, 0.9, 0.9], projection='astro mollweide')\n",
    "# Plot contours: 50% and 90% credible regions\n",
    "ax.contour_hpx(contour_map, levels=[p90, p50], colors=['red', 'black'], linewidths=1.5)\n",
    "    \n",
    "# Plot the HEALPix FITS map\n",
    "ax.imshow_hpx(filename, cmap=\"cylon\")\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192fac2-cf51-4336-af4e-932f20b400a3",
   "metadata": {},
   "source": [
    "This plot shows that the localization region is confined to a blob centered aroud RA of 13 h and Dec +30 degrees. In many real-life applications, alert \"brokers\" are setup to filter alerts with healpix partitioning of maps like the one above. However, we are interested in demonstrating the application of such maps, and will simplify the process by looking for alerts within a circular localization region centered on the blob.\n",
    "\n",
    "First, we will approximate a circular region that roughly encompasses about 80% of the probability region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccfe7ca-14a2-4f09-b7bf-98ebbc1f008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p80 = sorted_prob[np.searchsorted(cum_prob, 0.8)]\n",
    "\n",
    "pix_80 = np.where(prob >= p80)[0]\n",
    "ra_all_80, dec_all_80 = hp.pix2ang(nside, pix_80, lonlat=True)\n",
    "\n",
    "# Filter to northern hemisphere\n",
    "north_mask = ????\n",
    "\n",
    "ra_north_80 = ra_all_80[north_mask]\n",
    "dec_north_80 = dec_all_80[north_mask]\n",
    "\n",
    "# Create the SkyCoord array for the northern 90\n",
    "coords = SkyCoord(ra=ra_north_80 * u.deg, dec=dec_north_80 * u.deg, frame='icrs')\n",
    "\n",
    "# Convert to Cartesian unit vectors to compute an approximate mean\n",
    "cart = coords.cartesian\n",
    "mean_cart = cart.mean()\n",
    "center = SkyCoord(mean_cart, frame=coords.frame)\n",
    "print('Central sky coordinates', center)\n",
    "\n",
    "separations = coords.separation(center)\n",
    "max_radius = separations.max().to(u.degree).value\n",
    "print(f\"Max separation (80% region): {max_radius:.1f} degrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b2e4ed-2334-41a1-9623-c2369f61eedc",
   "metadata": {},
   "source": [
    "Our calculation shows that the edges of the 80% localization region are roughly 16 degrees in radius from the approximate central localization position. Lets see what that looks like on the sky projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ff7f93-d8fe-4d4e-acef-065beff0f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define axes with astro projection\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "ax = plt.axes([0.05, 0.05, 0.9, 0.9], projection='astro mollweide')\n",
    "# Plot contours: 50% and 90% credible regions\n",
    "ax.contour_hpx(contour_map, levels=[p90, p50], colors=['red', 'black'], linewidths=1.5)\n",
    "\n",
    "# Draw query region as a large transparent circle\n",
    "query_circle = mpatches.Circle(\n",
    "    (center.ra.degree, center.dec.degree),\n",
    "    radius=max_radius,\n",
    "    transform=ax.get_transform('world'),\n",
    "    facecolor='none',\n",
    "    edgecolor='cyan',\n",
    "    linewidth=2,\n",
    "    linestyle='--',\n",
    "    label=f'Radial Cone ({max_radius:.0f}\" radius)'\n",
    ")\n",
    "\n",
    "ax.add_patch(query_circle)\n",
    "ax.plot(\n",
    "    center.ra.degree,\n",
    "    center.dec.degree,\n",
    "    transform=ax.get_transform('world'),\n",
    "    marker='x',\n",
    "    color='cyan',\n",
    "    markersize=6,\n",
    "    label='Radial Center'\n",
    ")\n",
    "    \n",
    "# Plot the HEALPix FITS map\n",
    "ax.imshow_hpx(filename, cmap=\"cylon\")\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fb9505-6ba7-4931-abb9-498a4bd2bdb9",
   "metadata": {},
   "source": [
    "## Alert filtering using a broker\n",
    "\n",
    "\n",
    "Not bad for an approximate localization! This greatly simplifies our alert filtering procedure using public alert brokers which provide APIs to filter on cone searches. For this exercise, we will be using the [Alerce](https://alerce.science/) broker, which provides an easy python API to query alerts with spatial and temporal constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b2ba81-16c4-4a6a-aa69-bf7e76bd9baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Alerce()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbdddaa-211f-443b-a788-6ef3395df48f",
   "metadata": {},
   "source": [
    "Recall that ZTF produces few 100K alerts every night as part of this default survey. Even though we'll be only looking at public ZTF alerts, we have to be mindful with spatial and temporal constraints to ensure we are not flooded with irrelevant transients.\n",
    "\n",
    "Going ahead, we can apply three physically motivated constraints on the ZTF alert stream\n",
    "- We can assume that the associated transient is catastrophic in nature, i.e. the transient should have been detected for the first time in a short time interval *after* the event and remained active for a brief duration.\n",
    "- The transient should be spatially located within our approximated circular localization region\n",
    "- The transient should be detected in multiple epochs (at least 3) to avoid contamination from moving solar system objects.\n",
    "\n",
    "**Think: The requirement of 3 detections can be minimized to 2, but how is that related to the ZTF observing strategy?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b874a5-2859-4abc-a265-9f535c357187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by computing the MJD range corresponding to first 5 days after the GW event which was detected at 2019-05-21 03:02:29\n",
    "start_time = ????\n",
    "end_time = ????\n",
    "\n",
    "print(f\"Start MJD: {start_time}, End MJD: {end_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698d94aa-4856-45c8-99c6-7a27422f3352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets query Alerce!\n",
    "df = client.query_objects(\n",
    "    format='pandas',\n",
    "    ra=center.ra.degree,\n",
    "    dec=center.dec.degree,\n",
    "    radius=max_radius * 3600,  # in arcsec\n",
    "    firstmjd=[start_time, end_time],\n",
    "    page_size=10000,\n",
    "    ndet = [3, 99999]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(df)} objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b3a22a-6feb-4e01-a709-2871adbed49a",
   "metadata": {},
   "source": [
    "As you can see, even with these stringent constraints, one ends up with > 100 sources! The optical sky is extremely variable, and that makes these searches all the more challenging! \n",
    "\n",
    "Lets take a look at the list of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e66da-d638-4416-bf95-2a56d8810cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the contents of df\n",
    "?????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc78a09-3bb0-4e28-b905-c1b8b746639d",
   "metadata": {},
   "source": [
    "We see that some of the objects returned have mjdstarthist before the event! Why is that?\n",
    "\n",
    "It turns out that the public ZTF alert stream was initiated in mid-2018, which represents the start of operations of the alert brokers. However, ZTF did collect commissioning on-sky data prior to this date, which an create some prior alerts missing in the public archive but still correctly accounted in the ZTF alert schema.\n",
    "\n",
    "It is straightforward to remove these alerts by filtering on the pandas table directly. In addition, we don't want to be detecting variable stars, and we'll use the Alerce-issued \"stellar\" parameter to remove likely variable stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e862d1-cdac-47b4-a4a2-175b5efb2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a table called df_filtered that applies a cut on mjdstarthist and the stellar parameter\n",
    "df_filtered = ?????\n",
    "print(len(df_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b9d150-98be-43fc-9908-235c8a04cd6b",
   "metadata": {},
   "source": [
    "## Vetting of possible candidates\n",
    "\n",
    "Okay, we are down to <25 objects from > 1M alerts issued during this time period! At this stage, it is often useful to visually vet the light curves and cutouts for the objects. Remember that since we are working with an archival event, we have the LUXURY (in caps!) of knowing what these transients did over its lifetime. In real searches, one is only looking at the first few days of evolution, making it remarkably challenging!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb2cfb-1f6d-4640-82e8-7b7b21cd79d6",
   "metadata": {},
   "source": [
    "We'll define some functions to parse the Alerce light curve results and plot light curves and postage stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd9c9c-1d27-4e0c-997f-39c3ad772bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color config for filters\n",
    "colors = {1: \"green\", 2: \"red\"}\n",
    "labels = {1: 'g', 2: 'r'}\n",
    "markers = {1: 'o', 2: 's'}\n",
    "sizes = {1: 30, 2: 60}\n",
    "\n",
    "def plotStamps(oid, lc_det, client):\n",
    "    # Find first detection with a valid stamp\n",
    "    if \"has_stamp\" not in lc_det.columns or lc_det[\"has_stamp\"].sum() == 0:\n",
    "        print(f\"⚠️ No stamp available for {oid}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        candid = lc_det.loc[lc_det.has_stamp].sort_values(\"mjd\").candid.iloc[0]\n",
    "        stamps = client.get_stamps(oid, candid, format='HDUList')\n",
    "        science, ref, difference = stamps[0].data, stamps[1].data, stamps[2].data\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to fetch stamps for {oid}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Plot the cutouts\n",
    "    fig, ax = plt.subplots(ncols=3, figsize=(12, 5))\n",
    "    titles = [\"Science\", \"Reference\", \"Difference\"]\n",
    "    images = [science, ref, difference]\n",
    "\n",
    "    for i in range(3):\n",
    "        img = np.log1p(images[i])  # log scale with log1p for stability\n",
    "        ax[i].imshow(img, cmap='viridis', origin='lower')\n",
    "        ax[i].set_title(titles[i])\n",
    "        ax[i].axis(\"off\")\n",
    "\n",
    "    ax[0].set_title(f\"{oid}, candid: {candid}\", loc='left', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plotLC(oid, SN_det, SN_nondet):\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "    for fid in [1, 2]:\n",
    "        mask = SN_det.fid == fid\n",
    "        if np.sum(mask) > 0:\n",
    "            ax.errorbar(\n",
    "                SN_det[mask].mjd, SN_det[mask].magpsf,\n",
    "                yerr=SN_det[mask].sigmapsf,\n",
    "                c=colors[fid], label=labels[fid],\n",
    "                marker=markers[fid], linestyle='none'\n",
    "            )\n",
    "\n",
    "        mask = (SN_nondet.fid == fid) & (SN_nondet.diffmaglim > -900)\n",
    "        if np.sum(mask) > 0:\n",
    "            ax.scatter(\n",
    "                SN_nondet[mask].mjd, SN_nondet[mask].diffmaglim,\n",
    "                c=colors[fid], alpha=0.5, marker='v',\n",
    "                label=f\"lim.mag. {labels[fid]}\", s=sizes[fid]\n",
    "            )\n",
    "\n",
    "    # Add vertical marker at trigger time\n",
    "    mjd_marker = Time('2019-05-21 03:02:29', format='iso', scale='utc').mjd\n",
    "    ax.axvline(\n",
    "        mjd_marker, color='black', linestyle='--',\n",
    "        linewidth=1.5, label='Trigger Time (2019-05-21)'\n",
    "    )\n",
    "\n",
    "    ax.set_title(oid, fontsize=20)\n",
    "    ax.set_xlabel(\"MJD\", fontsize=16)\n",
    "    ax.set_ylabel(\"Apparent magnitude\", fontsize=16)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(ax.get_ylim()[::-1])\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def getSNdata(oid, doLC=False, doStamps=False):\n",
    "    results = {\"oid\": oid}\n",
    "\n",
    "    try:\n",
    "        lc_det = client.query_detections(oid, format='pandas').sort_values(\"mjd\")\n",
    "        results[\"lc_det\"] = lc_det\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not fetch detections for {oid}: {e}\")\n",
    "        lc_det = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        lc_nondet = client.query_non_detections(oid, format='pandas').sort_values(\"mjd\")\n",
    "        results[\"lc_nondet\"] = lc_nondet\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not fetch non-detections for {oid}: {e}\")\n",
    "        lc_nondet = pd.DataFrame()\n",
    "\n",
    "    # Plot light curve\n",
    "    if doLC and not lc_det.empty and not lc_nondet.empty:\n",
    "        plotLC(oid, lc_det, lc_nondet)\n",
    "\n",
    "    # Plot stamps\n",
    "    if doStamps and not lc_det.empty:\n",
    "        plotStamps(oid, lc_det, client)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16829398-2087-466b-bedd-19b2d9627885",
   "metadata": {},
   "source": [
    "Now we are ready to start plotting some light curves and postage stamps! For clarity, we are also plotting a line indicating the time of the GW trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708b44a-1439-4a18-8889-701d524f82ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for oid in df_filtered[\"oid\"].unique():\n",
    "    print(f\"Plotting light curve for {oid}\")\n",
    "    getSNdata(oid, doLC=True, doStamps=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f640b0-022f-48bf-8f81-b194deb7246e",
   "metadata": {},
   "source": [
    "As we see above, the transients identified span a wide variety of colors and timescales. In real-time filtering, one can apply additional filters such as evidence for a fast decaying light curve and red colors -- as expected of kilonovae. From visual vetting of the light curves, we see that all the events are long-lived (> 2 weeks), which would eventually rule all of them out in a search. \n",
    "\n",
    "Ultimately, this is a binary black hole merger event, so it is not suprising that we did not find a kilonova counterpart! \n",
    "\n",
    "**Now imagine the era of LSST, with the vastly large number of alerts (> 100X) per unit area on sky, is it still feasible to do untargeted searches like this? How do we narrow down to a feasible list of candidates?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562f856-b4eb-45b6-b4bb-d8479dce5ad8",
   "metadata": {},
   "source": [
    "## Using external catalogs to aid discovery\n",
    "\n",
    "Remember at the start of the notebook, we said that there was indeed a counterpart claimed for this event. As we will hear during the school, there are physically motivated models that suggest that BH-BH mergers inside AGN disks may indeed lead to electromagnetic counterparts. In this case, the timescales are likely longer due to the massive nature of the BHs. We can try to demonstrate such a search together with the utility of external catalogs in such searches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3789a8b9-f2e7-4e4d-99c5-e0905eb53e9f",
   "metadata": {},
   "source": [
    "Let's start by repeating the Alerce query -- this time with a longer time window extending an additional 1 month beyond our original search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f641f1-5ac9-4722-8390-00bb4162fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create revised MJD range with an additional 30 days\n",
    "start_time = ?????\n",
    "end_time = ????? \n",
    "\n",
    "print(f\"Start MJD: {start_time}, End MJD: {end_time}\")\n",
    "\n",
    "#Lets query Alerce using the same format as above, but with the modified time range\n",
    "df = ?????\n",
    "\n",
    "print(f\"Found {len(df)} objects\")\n",
    "\n",
    "#repeat filtering for old objects\n",
    "df_filtered = ?????\n",
    "print(len(df_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6550a-3fdb-4948-b11d-d02cedb6c9ab",
   "metadata": {},
   "source": [
    "Because our model predictions suggest that such objects should be in AGN disks, we can further cut down by crossmatching to an external catalog, such as the [Milliquas](https://ui.adsabs.harvard.edu/abs/2023OJAp....6E..49F/abstract) catalog on Vizier. One could also do a similar query say to crossmatch cnadidates against a catalog of galaxies if relevant. To demonstrate, let's crossmatch our sources to this catalog on Vizier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c638c66-d0ff-4a99-8c71-55751e821e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = \"VII/294\" #Milliquas catalog ID\n",
    "radius = 2 * u.arcsec\n",
    "agn_oids = []\n",
    "agn_zs = []\n",
    "for index, row in df_filtered.iterrows():\n",
    "    ra = row['meanra']\n",
    "    dec = row['meandec']\n",
    "    oid = row['oid']\n",
    "\n",
    "    coord = SkyCoord(\n",
    "        ra=ra*u.deg,\n",
    "        dec=dec*u.deg,\n",
    "        frame='icrs'\n",
    "    )\n",
    "\n",
    "    result = Vizier.query_region(coord, radius=radius, catalog=catalog)\n",
    "    if result:\n",
    "        agn_name = result[0]['Name'].value[0]\n",
    "        z = result[0]['z'].value[0]\n",
    "        print(f\"Source {oid} crossmatched to AGN {agn_name} at redshift {z}\") \n",
    "\n",
    "        ###update the agn_oids and agn_zs array for things that have a crossmatch\n",
    "        ?????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b904f-5654-461e-85b5-1b840259bca8",
   "metadata": {},
   "source": [
    "And there we go, we were able to narrow down to < 20 objects using an external catalog crossmatch. A key advantage of assuming such an association between the transient and an external catalog is that we can draw on the information in the external catalog (e.g. redshift) to apply additional physically motivated filters. \n",
    "\n",
    "While we haven't used this yet, let's take a look at the distance information available in the GW map and overlay the likely distances of the candidate counterparts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca67ba-3616-4de6-8783-56f61f11b825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-finite or zero probabilities\n",
    "valid = (prob > 0) & np.isfinite(distmu) & np.isfinite(distsigma) & (distmu > 0)\n",
    "\n",
    "# Convert redshift to luminosity distance\n",
    "cosmo = FlatLambdaCDM(H0=70, Om0=0.3)\n",
    "agn_dists = cosmo.luminosity_distance(agn_zs).value  # in Mpc\n",
    "\n",
    "# Create weighted histogram of distances using distmu, weighted by prob\n",
    "plt.hist(distmu[valid], weights=prob[valid], bins=50, histtype='step', color='black')\n",
    "\n",
    "# Add vertical lines and labels\n",
    "for oid, dist in zip(agn_oids, agn_dists):\n",
    "    plt.axvline(dist, color='red', linestyle='--', linewidth=0.8)\n",
    "    plt.text(dist, plt.ylim()[1]*0.05, oid, rotation=90, verticalalignment='bottom', fontsize=7, color='red')\n",
    "\n",
    "plt.xlabel('Distance (Mpc)')\n",
    "plt.ylabel('Probability-weighted count')\n",
    "plt.title('Distance Distribution of GW Localization')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba0722-dd9a-4fb1-b31b-a01227fa4267",
   "metadata": {},
   "source": [
    "Here we see that most of the candidates identified with a pure 2D spatial search are in fact outside the 3D localization volume! We can use this information to further filter for candidates that have luminosity distances between about 2 - 5 Gpc (see map above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e0ec7-f43c-4681-a2eb-f938b5def33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_low_z = z_at_value(cosmo.luminosity_distance, 2 * u.Gpc)\n",
    "lim_high_z = z_at_value(cosmo.luminosity_distance, 5 * u.Gpc)\n",
    "\n",
    "final_agn_cands = []\n",
    "for i in range(len(agn_oids)):\n",
    "    # apply a criteria on agn_zs to filter candidates within the desired redshift range and save them in final_agn_cands\n",
    "    ??????\n",
    "    \n",
    "print('Remaining %d candidates in crossmatch'%len(final_agn_cands))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6990c9a-9a63-4c64-81d7-66bb38cba5a5",
   "metadata": {},
   "source": [
    "We are left with just 7 sources! At this stage, the number is small enough that we can repeat our visual vetting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e68f4-831b-4439-a6af-d1ff67fda84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot their light curves\n",
    "for oid in final_agn_cands:\n",
    "    ?????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7fc040-98bf-4c21-aae1-ce5e205dbfb0",
   "metadata": {},
   "source": [
    "Remember again that here we have the luxury of \"knowing\" the future evolution of all the sources since the time of the trigger. Visually inspect these light curves, does any one stand out? \n",
    "\n",
    "We expect even BBH mergers in AGN disks to be catastrophic events (or possibly extremely long recurrence times; see [Graham et al. 2021](https://ui.adsabs.harvard.edu/abs/2020PhRvL.124y1102G/abstract)), implying that the transient should be a one-time appearance instead of the general burbling that AGN are known to do otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe1e78-8a7c-4fb0-a15b-e6a8305b8d9b",
   "metadata": {},
   "source": [
    "Upon careful inspection, one sees that the source **ZTF19abanrhr** is distinctive in this population in having showed a single flare around the time of the event, while others have exhibited continuous variability. This is in fact the candidate claimed by [Graham et al. 2021](https://ui.adsabs.harvard.edu/abs/2020PhRvL.124y1102G/abstract). **We have just re-discovered it using few lines of code!** For interested readers, it will be worth going through that paper to understand the use the additional archival information from prior time domain surveys that further supported an association. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92973a-4ab6-44ea-8290-2caf9798157a",
   "metadata": {},
   "source": [
    "The use of three dimensional information as well as external catalogs helps us dramatically reduce the number of plausible candidates -- and particularly powerful in the era of massively multiplexed spectroscopic surverys. This is **going to be critical in the era of LSST** where the substantially larger depth will produce $> 200$ times more transients per unit area on the sky."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56992899-c811-429c-aa6c-9ac7a69af558",
   "metadata": {},
   "source": [
    "# Subject for discussion: Broader ideas on how to filter for associations? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
